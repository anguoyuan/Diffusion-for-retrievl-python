{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6oTD/91UbdXwBIHzGtV6g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anguoyuan/Diffusion-for-retrievl-python/blob/main/diffusion_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a tutorial about diffusion for image retrieval instead of generation. It is mainly based on Iscen's CVPR17 [paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Iscen_Efficient_Diffusion_on_CVPR_2017_paper.pdf) and the[ code](https://github.com/ducha-aiki/manifold-diffusion) of Dmytro Mishkin. Beginners might find Fan Yang's [slides](https://github.com/fyang93/diffusion/blob/master/slides.pdf) useful for understanding diffusion (but its implementation sometimes is not as effective as the following standard one). Please use the following code if you prefer high accuracy.  \n",
        "\n",
        "## Key Insights on Diffusion\n",
        "\n",
        "* Diffusion, a widely recognized reranking technique in retrieval, has proven its efficacy across numerous datasets and features.\n",
        "\n",
        "* Diffusion can enhance retrieval performance without additional training.\n",
        "\n",
        "* A notable variant of diffusion is PageRank, a technique that played a key role in Google's significant business success.\n",
        "\n",
        "* Although traditionally reranking might increase time costs, recent studies have successfully transitioned the process to an offline mode, thereby significantly minimizing online costs. For further understanding, refer to the [NeurIPS Hypergraph](https://github.com/anguoyuan/Hypergraph-Propagation-and-Community-Selection-for-Objects-Retrieval) and [AAAI offline diffusion](https://github.com/fyang93/diffusion/tree/master) studies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qYGIe0kOgGGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy.sparse import csr_matrix, eye, diags\n",
        "from scipy.sparse import linalg as s_linalg\n",
        "from time import time"
      ],
      "metadata": {
        "id": "xfRHW6LyAJ0o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, prepare some useful functions.\n",
        "\n",
        "* sim_kernel: A commonly used similarity kernel in the retrieval field. For more details, refer to the \"Regional Diffusion\" paper by Iscen, in Section 5.1.\n",
        "\n",
        "* normalize_connection_graph: This function computes the symmetric normalization of the affinity matrix, aiding in data structure interpretation.\n",
        "\n",
        "* topK_W: This function change a full affinity graph into a k-NN graph. Since the original affinity matrix can become unmanageable due to its quadratic relation to the number of vectors in the database, this approach is necessary. Truncation to a k-NN structure maintains nearly the same level of accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S3S5WOEAhpi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_kernel(dot_product):\n",
        "    return np.maximum(np.power(dot_product,3),0)\n",
        "\n",
        "def normalize_connection_graph(G):\n",
        "    W = csr_matrix(G)\n",
        "    W = W - diags(W.diagonal()) # sets the diagonal entries of the matrix W to zero. zero self-similarities\n",
        "    D = np.array(1./ np.sqrt(W.sum(axis = 1))) # degree matrix of the graph. It is the normalization factor, the content is inverse of the squre root of the sum of each row of W,\n",
        "    D[np.isnan(D)] = 0\n",
        "    D[np.isinf(D)] = 0\n",
        "    D_mh = diags(D.reshape(-1))\n",
        "    Wn = D_mh * W * D_mh #normalization\n",
        "    return Wn\n",
        "\n",
        "def topK_W(G, K = 100):\n",
        "    sortidxs = np.argsort(-G, axis = 1)\n",
        "    for i in range(G.shape[0]):\n",
        "        G[i,sortidxs[i,K:]] = 0\n",
        "    G = np.minimum(G, G.T)\n",
        "    return G"
      ],
      "metadata": {
        "id": "ia7M0BhpgT98"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the initial search result."
      ],
      "metadata": {
        "id": "A5JsBEa_huWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.random.rand(2048,4996)\n",
        "Q=np.random.rand(2048,70)"
      ],
      "metadata": {
        "id": "LxF8hVsqbzui"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 100 # approx 50 mutual nns\n",
        "QUERYKNN = 10\n",
        "R = 2000\n",
        "alpha = 0.9"
      ],
      "metadata": {
        "id": "nrCZOAdfcwf1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim  = np.dot(X.T, Q)\n",
        "qsim = sim_kernel(sim).T\n",
        "sortidxs = np.argsort(-qsim, axis = 1)\n",
        "for i in range(len(qsim)):\n",
        "    qsim[i,sortidxs[i,QUERYKNN:]] = 0"
      ],
      "metadata": {
        "id": "M8BOzSIOhM69"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the normalized laplacian matrix"
      ],
      "metadata": {
        "id": "zENtxGW1h2qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qsim = sim_kernel(qsim)\n",
        "A = np.dot(X.T, X)\n",
        "W = sim_kernel(A).T\n",
        "W = topK_W(W, K)\n",
        "Wn = normalize_connection_graph(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InpStq8YhUu8",
        "outputId": "680e90ee-bb78-40f2-c921-eb7fbd9d9831"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-fc2c72ad4698>:7: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  D = np.array(1./ np.sqrt(W.sum(axis = 1))) # degree matrix of the graph. It is the normalization factor, the content is inverse of the squre root of the sum of each row of W,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try the diffusion. We use MINimum RESidual iteration to solve *(I-αWn)f=b*, where *(I-αWn)* is the Laplacian of the normalized affinity matrix, *b* is the initial ranking result for each query, and the solved *f* is the result of diffusion.  "
      ],
      "metadata": {
        "id": "vj7WZKcXh-Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cg_diffusion(qsims, Wn, alpha = 0.99, maxiter = 10, tol = 1e-3):\n",
        "    Wnn = eye(Wn.shape[0]) - alpha * Wn # Laplacian matrix\n",
        "    out_sims = []\n",
        "    for i in range(qsims.shape[0]):\n",
        "        #f,inf = s_linalg.cg(Wnn, qsims[i,:], tol=tol, maxiter=maxiter)\n",
        "        f,inf = s_linalg.minres(Wnn, qsims[i,:], tol=tol, maxiter=maxiter) #There are several famous solvers for the linear system. Here, it uses MINimum RESidual iteration instead of Conjugate Gradient\n",
        "        out_sims.append(f.reshape(-1,1))\n",
        "    out_sims = np.concatenate(out_sims, axis = 1)\n",
        "    ranks = np.argsort(-out_sims, axis = 0)\n",
        "    return ranks\n"
      ],
      "metadata": {
        "id": "olFXcagthekb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cg_ranks =  cg_diffusion(qsim, Wn, alpha)"
      ],
      "metadata": {
        "id": "pw4k0ahjhkGl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cg_ranks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55gE3WPudALg",
        "outputId": "85aff345-e066-42b9-9894-24a12fff7286"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4996, 70)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cases where the database is excessively large, an effective approach is to truncate the affinity matrix. This is achieved by retaining only the rows and columns pertaining to the top-ranked image regions and subsequently re-normalizing the matrix.\n",
        "\n",
        "It's important to note that this process is distinct from topK_W, which is responsible for the construction of the k-NN graph.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UBM0rDPtiBWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_trunc_graph(qs, W, levels = 3):\n",
        "    needed_idxs = []\n",
        "    needed_idxs = list(np.nonzero(qs > 0)[0])\n",
        "    for l in range(levels):\n",
        "        idid = W.nonzero()[1]\n",
        "        needed_idxs.extend(list(idid))\n",
        "        needed_idxs =list(set(needed_idxs))\n",
        "    return np.array(needed_idxs), W[needed_idxs,:][:,needed_idxs]\n",
        "\n",
        "def dfs_trunk(sim, A,alpha = 0.99, QUERYKNN = 10, maxiter = 8, K = 100, tol = 1e-3):\n",
        "    qsim = sim_kernel(sim).T\n",
        "    sortidxs = np.argsort(-qsim, axis = 1)\n",
        "    for i in range(len(qsim)):\n",
        "        qsim[i,sortidxs[i,QUERYKNN:]] = 0\n",
        "    qsims = sim_kernel(qsim)\n",
        "    W = sim_kernel(A)\n",
        "    W = csr_matrix(topK_W(W, K))\n",
        "    out_ranks = []\n",
        "    t =time()\n",
        "    for i in range(qsims.shape[0]):\n",
        "        qs =  qsims[i,:]\n",
        "        tt = time()\n",
        "        w_idxs, W_trunk = find_trunc_graph(qs, W, 2);\n",
        "        Wn = normalize_connection_graph(W_trunk)\n",
        "        Wnn = eye(Wn.shape[0]) - alpha * Wn\n",
        "        f,inf = s_linalg.minres(Wnn, qs[w_idxs], tol=tol, maxiter=maxiter)\n",
        "        ranks = w_idxs[np.argsort(-f.reshape(-1))]\n",
        "        missing = np.setdiff1d(np.arange(A.shape[1]), ranks)\n",
        "        out_ranks.append(np.concatenate([ranks.reshape(-1,1), missing.reshape(-1,1)], axis = 0))\n",
        "    #print time() -t, 'qtime'\n",
        "    out_ranks = np.concatenate(out_ranks, axis = 1)\n",
        "    return out_ranks\n"
      ],
      "metadata": {
        "id": "aQS3DlcWiL5M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cg_trunk_ranks =  dfs_trunk(sim, A, alpha = alpha, QUERYKNN = QUERYKNN )"
      ],
      "metadata": {
        "id": "5ZEpA5t6iFL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc6cbe5-0a2e-4494-83dc-383627ef5277"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-fc2c72ad4698>:7: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  D = np.array(1./ np.sqrt(W.sum(axis = 1))) # degree matrix of the graph. It is the normalization factor, the content is inverse of the squre root of the sum of each row of W,\n"
          ]
        }
      ]
    }
  ]
}